# Search

Agent: entity perceive environment and acts upon

State: agent in the environment

Initial State

Actions: returns the set of actions that can be executed

Transition model:  returns the state resulting from performing action a in state s

State space: All possible state from initial state

Goal test: way  to determine a given state is goal

Path cost: numerical cost associated with a given path

Solution: a sequence of actions from initial to goal

Optimal solution: solution with lowest path cost 

node:  a data structure keep track of



*   State
*   Parent (node that generated this node)
*   Action (action from parent to current node)
*   Path cost

Approach:

Start with a frontier that contain initial state

Repeat:



*   If frontier is empty no solution
*   Remove a node from frontier
*   If node contains goal, return the solution
*   Expand node, add resulting node to frontier


## Uninformed search

Search strategy that uses no problem specific knowledge


## Informed search

Search strategy that uses problem-specific knowledge to find solutions more efficiently.


## Greedy BFS

Search algorithm that expands the node that is closest to the goal, as estimated by heuristic function h(n)



*   Manhattan Distance
*   Coming up good heuristic can be challenging
*   Not always optimal, making local optimal, but not global


## A* Search

Expands node with lowest value of g(n) + h(n)

g(n) - cost to reach node

h(n) - estimated cost to goal

Optimal if



*   h(n) is admissible - never overestimates the true cost
*   h(n) is consistent - for node n and successor n’ and cost c, h(n) &lt;= h(n’) + c


## Adversarial Search



*   I try to succeed, someone try to stop me, e.g. PVP games


## Minimax (example, tic-tac-toe)

Max Player - max(x) aiming to maximize score

Min Player - min(o) aiming to minimize score

S0 - initial state

player(s): returns which player to move in state s

actions(s): returns legal moves in state s

result(s, a): returns state after taking action a in state s

terminal(s): end state

utility(s): final numerical value for terminal state s

Given state s:



*   MAX player picks action a in Actions(s) that produces highest value of Min-Value(Result(s, a))
*   MIN player picks action a Actions(s) that prudces Max-Value(Result(s, a))

Function Max-Value(state):

    If terminal(state):

        Return utility(state)

    Else:

       Return max([Min-Value(Result(state, action)) for action in Actions(state)])

    


### Optimization


### Alpha-Beta Pruning

Keeping track and check against current optimal in order to reduce nodes examined.


### Depth-Limited Minimax

Check min-max scores for a certain depth


#### Evaluation Function

function that estimates the expected utility of the game from a given state


# Knowledge


## Knowledge-based agents

Agents that reason by operating on internal representations of knowledge


## Sentence

An assertion about the world in a knowledge representation language


## Propositional Logic


### Propositional Symbols

P, Q, R


### Logical Connectives

Not ¬, and ∧, or ∨, implication →, biconditional ↔

Implication - if P is true, Q is true, P implies Q, if P is false, Q can be either

Biconditional - if and only if


### Model

Assignment of a truth value to every propositional symbols


### Knowledge base

A set of sentence known by knowledge-based agent


### Entailment

α ⊨ β

In every model in which sentence α is true, sentence β is also true.


### Inference

The process of deriving new sentences from old ones


## Inference Algorithms

KB ⊨ α


### Model Checking



*   Enumerate all possible models
*   If in every model where KB is true, α is true, then KB entails α
*   Otherwise KB does not entail α


### Inference Rules


#### Modus Ponens

If α → β and α  is true, then β is true


#### And Elimination

If α ∧ β is true, α is true and β is true


#### Double Negation Elimination

¬(¬α) = α


#### Implication Elimination

α → β = ¬α ∨ β 


#### Biconditional Elimination

α ↔ β = (α → β) ∧ (β → α)


#### De Morgan’s Law

Turn and into or

¬(α ∧ β) = ¬α ∨ ¬β 

¬(α ∨ β) = ¬α ∧ ¬β


#### Distributive Property

(α ∨ (β ∧ γ))  = (α ∨ β) ∧ (α ∨ γ)

(α ∧ (β ∨ γ)) = (α ∧ β) ∨ (α ∧ γ)


#### Theorem Proving as search problem

initial state: starting knowledge base 

actions: inference rules 

transition model: new knowledge base after inference 

goal test: check statement we're trying to prove 

path cost function: number of steps in proof


#### Resolution

If P ∨ Q is true, ¬P is true, then Q is true

If P ∨ Q is true, ¬P ∨ R is true, then Q ∨ R is true


##### Clause

A disjunction of literals 



*   disjunction, things connected with or; conjunction, things connected with and
*   Literals, either a “symbol” or “not symbol”


##### Conjunctive normal form (CNF)

Logical sentence that is a conjunction of clauses, work as input for resolution

Convert any logical sentence to CNF with influence rules



*   Eliminate biconditionals
*   Eliminate implications
*   Move not inwards using De Morgan’s Laws
*   Use distributive law to distribute or wherever possible

Inference by Resolution

To determine if KB ⊨ α: 



*   Check if (KB ∧ ¬α) is a contradiction? 
*   If so, then KB ⊨ α. 
*   Otherwise, no entailment
*   Convert (KB ∧ ¬α) to Conjunctive Normal Form. • 
*   Keep checking to see if we can use resolution to produce a new clause. • 
*   If ever we produce the empty clause (equivalent to False), we have a contradiction, and KB ⊨ α. •
*    Otherwise, if we can't add new clauses, no entailment


## First Order Logic

Constant Symbol - Object

Predicate Symbol - Function


### Universal Quantification

∀ For all values of x, something is true


### Existential Quantification

∃ For at least 1 values of x, something is true.


## Knowledge Engineering



*   Take a real world problem and transfer into knowledge problem


# Uncertainty


## Probability Theory

**Possible Worlds** - w

**Probability** - P(w) in range of [0, 1]

Sum(P(w)) for all w in W(all possible worlds) = 1

**Unconditional Probability**

Degree of belief in a proposition in the absence of any other evidence

**Conditional Probability**

Degree of belief in a proposition given some evidence that has already been revealed

P(a|b) - Probability of a given b is true

P(a|b) = P(a, b) / P(b)

P(a and b) = P(b) * P(a|b) = P(a) * P(b|a)



*   P(a,b) = P(a and b)

**Random Variable**

A variable in probability theory with a domain of possible values it can take on

**Probability distribution**

P(Flight) = &lt;0.6, 0.3, 0.1>



*   P(Flight = on time) = 0.6
*   P(Flight = delayed) = 0.3
*   P(Flight = cancelled) = 0.1

**Independence**

The knowledge of one event does not affect probability of the other event

P(a and b) = P(a) * P(b|a) = P(a) * P(b)

**Bayes’ Rule**

P(b|a) = P(a|b) * P(b) / P(a)

**Joint Probability**

P(a|b) = P(a, b) / P(b)

Conditional Probability is proportional to Joint Probability


## Probability Rules

**Negation**

P(¬a) = 1 - P(a)

**Inclusion-Exclusion**

P(a ∨ b) = P(a) + P(b) - P(a, b)

**Marginalization**

P(a) = P(a, b) + P(a, ¬b)

P(X = xi ) = ∑ j P(X = xi , Y = yj )

**Conditioning**

P(a) = P(a|b)P(b) + P(a|¬b)P(¬b)

P(X = xi ) = ∑ j P(X = xi|Y = yj )P(Y = yj )

**Bayesian network**

data structure that represents the dependencies among random variables

• directed graph 

• each node represents a random variable 

• arrow from X to Y means X is a parent of Y 

• each node X has probability distribution P(X | Parents(X))


### Inference

Query X: variable  for which to compute distribution

Evidence variables E: observed variables for event e

Hidden variables Y: Non-evidence, Non-query variable.

Goal: Calculate P(X|e)

**Inference by Enumeration **

P(X | e) = α P(X, e) = α ∑y P(X, e, y)

X is the query variable. 

e is the evidence. 

y ranges over values of hidden variables. 

α normalizes the result


### Approximate Inference

**Sampling/Rejection Sampling**

monte carlo simulation?

**Likelihood Weighting**

• Start by fixing the values for evidence variables. 

• Sample the non-evidence variables using conditional probabilities in the Bayesian Network. 

• Weight each sample by its likelihood: the probability of all of the evidence


### Uncertainty over Time

**Markov assumption**

the assumption that the current state depends on only a finite fixed number of previous states

**Markov chain**

a sequence of random variables where the distribution of each variable follows the Markov assumption


#### Sensor Models

**filtering** given observations from start until now, calculate distribution for current state 

**prediction** given observations from start until now, calculate distribution for a future state 

**smoothing** given observations from start until now, calculate distribution for past state 

**most likely explanation** given observations from start until now, calculate most likely sequence of states

**Hidden Markov Model**

a Markov model for a system with hidden states that generate some observed event

**sensor Markov assumption **

the assumption that the evidence variable depends only the corresponding state


# Optimization

choosing the best option from a set of options


## Local Search

search algorithms that maintain a single node and searches by moving to a neighboring node



*   State-Space landscape
*   An Objective Function is a function that we use to maximize the value of the solution.
*   A Cost Function is a function that we use to minimize the cost of the solution (this is the function that we would use in our example with houses and hospitals. We want to minimize the distance from houses to hospitals).
*   A Current State is the state that is currently being considered by the function.
*   A Neighbor State is a state that the current state can transition to. In the one-dimensional state-space landscape above, a neighbor state is the state to either side of the current state. In our example, a neighbor state could be the state resulting from moving one of the hospitals to any direction by one step. Neighbor states are usually similar to the current state, and, therefore, their values are close to the value of the current state.


### Hill Climbing

Repeatedly pick neighbour with a better score, until no better neighbours



*   stuck at local maxima, minima, shoulder


#### Variants

**Steepest-ascent**

choose the highest-valued neighbor 

**stochastic **

choose randomly from higher-valued neighbors 

**first-choice **

choose the first higher-valued neighbor 

**random-restart **

conduct hill climbing multiple times 

**local beam **

search chooses the k highest-valued neighbors, unlike most local search algorithms in that it uses multiple nodes for the search, and not just one.


## Simulated Annealing

• Early on, higher "temperature": more likely to accept neighbors that are worse than current state

• Later on, lower "temperature": less likely to accept neighbors that are worse than current state


## Linear Programming

Linear programming is a family of problems that optimize a linear equation (an equation of the form y = ax₁ + bx₂ + …).

Linear programming will have the following components:



*   A cost function that we want to minimize: c₁x₁ + c₂x₂ + … + cₙxₙ. Here, each x₋ is a variable and it is associated with some cost c₋.
*   A constraint that’s represented as a sum of variables that is either less than or equal to a value (a₁x₁ + a₂x₂ + … + aₙxₙ ≤ b) or precisely equal to this value (a₁x₁ + a₂x₂ + … + aₙxₙ = b). In this case, x₋ is a variable, and a₋ is some resource associated with it, and b is how much resources we can dedicate to this problem.
*   Individual bounds on variables (for example, that a variable can’t be negative) of the form lᵢ ≤ xᵢ ≤ uᵢ.

**Efficient Algorithms for Linear Programming**



*   Simplex
*   Interior-Point


# Constraint Satisfaction

Constraint Satisfaction problems are a class of problems where variables need to be assigned values while satisfying some conditions.

Constraints satisfaction problems have the following properties:



*   Set of variables (x₁, x₂, …, xₙ)
*   Set of domains for each variable {D₁, D₂, …, Dₙ}
*   Set of constraints C
*   A Hard Constraint is a constraint that must be satisfied in a correct solution.
*   A Soft Constraint is a constraint that expresses which solution is preferred over others.
*   A Unary Constraint is a constraint that involves only one variable. In our example, a unary constraint would be saying that course A can’t have an exam on Monday {_A ≠ Monday_}.
*   A Binary Constraint is a constraint that involves two variables. This is the type of constraint that we used in the example above, saying that some two courses can’t have the same value {_A ≠ B_}.

**node consistency**

Node consistency is when all the values in a variable’s domain satisfy the variable’s unary constraints.

**arc consistency**

Arc consistency is when all the values in a variable’s domain satisfy the variable’s binary constraints


<table>
  <tr>
   <td>function Revise(<em>csp, X, Y</em>):
<ol>

<li><em>revised</em> = <em>false</em>

<li>for <em>x</em> in <em>X.domain</em>: 
<ol>
 
<li>if no <em>y</em> in <em>Y.domain</em> satisfies constraint for (<em>X,Y</em>):  
<ol>
  
<li>delete <em>x</em> from <em>X.domain</em>
  
<li><em>revised</em> = true
</li>  
</ol>
</li>  
</ol>

<li>return <em>revised</em>
</li>
</ol>
   </td>
  </tr>
  <tr>
   <td>function AC-3(<em>csp</em>):
<ol>

<li><em>queue</em> = all arcs in <em>csp</em>

<li>while <em>queue</em> non-empty: 
<ol>
 
<li>(<em>X, Y</em>) = Dequeue(<em>queue</em>)
 
<li>if Revise(<em>csp, X, Y</em>):  
<ol>
  
<li>if size of <em>X</em>.domain == 0:   
<ol>
   
<li>return <em>false</em>
</li>   
</ol>
  
<li>for each <em>Z</em> in <em>X</em>.neighbors - {<em>Y</em>}:   
<ol>
   
<li>Enqueue(queue, (<em>Z,X</em>))
</li>   
</ol>
</li>   
</ol>
</li>   
</ol>

<li>return true
</li>
</ol>
   </td>
  </tr>
</table>


While the algorithm for arc consistency can simplify the problem, it will not necessarily solve it, since it considers binary constraints only and not how multiple nodes might be interconnected


## CSP as Search Problem

• initial state: empty assignment (no variables) 

• actions: add a {variable = value} to assignment 

• transition model: shows how adding an assignment changes the assignment 

• goal test: check if all variables assigned and constraints all satisfied 

• path cost function: all paths have same cost


### Backtracking Search


```
function Backtrack(assignment, csp):
if assignment complete:
return assignment
var = Select-Unassigned-Var(assignment, csp)
for value in Domain-Values(var, assignment, csp):
if value consistent with assignment:
add {var = value} to assignment
result = Backtrack(assignment, csp)
if result ≠ failure:
return result
remove {var = value} from assignment
return failure
```



### Inference: Maintaining Arc-Consistency algorithm

algorithm for enforcing arc-consistency every time we make a new assignment



*   When we make a new assignment to X, calls AC-3, starting with a queue of all arcs (Y, X) where Y is a neighbor of X

```
function Backtrack(assignment, csp):
if assignment complete:
return assignment
var = Select-Unassigned-Var(assignment, csp)
for value in Domain-Values(var, assignment, csp):
if value consistent with assignment:
add {var = value} to assignment
inferences = Inference(assignment, csp)
if inferences ≠ failure:
add inferences to assignment
result = Backtrack(assignment, csp)
if result ≠ failure:
return result
remove {var = value} and inferences from assignment
return failure
```


**SELECT-UNASSIGNED-VAR **

• minimum remaining values (MRV) heuristic: select the variable that has the smallest domain 

• degree heuristic: select the variable that has the highest degree, (most popular node)

**Domain-Values**

• least-constraining values heuristic: return variables in order by number of choices that are ruled out for neighboring variables 

• try least-constraining values first


# Machine Learning


## supervised learning 

given a data set of input-output pairs, learn a function to map inputs to outputs


### classification

supervised learning task of learning a function mapping an input point to a discrete category

**nearest-neighbor classification **

algorithm that, given an input, chooses the class of the nearest data point to that input

**k-nearest-neighbor classification **

algorithm that, given an input, chooses the class of the nearest data point to that input

**linear regression**

Weight Vector w: (w0, w1, w2) 

Input Vector x: (1, x1, x2) 

dot product w · x: w0 + w1x1 + w2x2

h(x1, x2) = 1 if w0 + w1x1 + w2x2 ≥ 0; 0 otherwise

**perceptron learning rule** 

Given data point (x, y), update each weight according to: 

wi = wi + α(y - hw(x)) × xi

**support vector machine**

**maximum margin separator**

boundary that maximizes the distance between any of the data points


### regression

supervised learning task of learning a function mapping an input point to a continuous value


### Evaluating Hypotheses

**loss function **

function that expresses how poorly our hypothesis performs

**0-1 loss function** 

L(actual, predicted) = 0 if actual = predicted, 1 otherwise

**L1 loss function **

L(actual, predicted) = | actual - predicted |

**L2 loss function : **Penalize wrong prediction

L(actual, predicted) = (actual - predicted)2

**overfitting **

a model that fits too closely to a particular data set and therefore may fail to generalize to future data

**regularization **

penalizing hypotheses that are more complex to favor simpler, more general hypotheses 

cost(h) = loss(h) + λcomplexity(h)

**holdout cross-validation**

 splitting data into a training set and a test set, such that learning happens on the training set and is evaluated on the test set

**k-fold cross-validation** 

splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data as training set

**python lib: scikit-learn**


### Reinforcement Learning

given a set of rewards or punishments, learn what actions to take in the future

**Markov Decision Process - extension of markov chain**

model for decision-making, representing states, actions, and their rewards

•Set of states S 

•Set of actions ACTIONS(s) 

•Transition model P(s' | s, a) 

•Reward function R(s, a, s')

**Q-Learning**

method for learning a function Q(s, a), estimate of the value of performing action a in state s

• Start with Q(s, a) = 0 for all s, a 

• When we taken an action and receive a reward: 


    • Estimate the value of Q(s, a) based on current reward and expected future rewards 


    • Update Q(s, a) to take into account old estimate as well as our new estimate

• Every time we take an action a in state s and observe a reward r, we update: 

Q(s, a) ← Q(s, a) + α(new value estimate - Q(s, a))

	← Q(s, a) + α((r + future reward estimate) - Q(s, a))

	← Q(s, a) + α((r + max_a' Q(s', a')) - Q(s, a))

**Greedy Decision-Making **

•When in state s, choose action a with highest Q(s, a)


#### Explore vs. Exploit

explore - discover new knowledge

exploit - use old knowledge

**ε-greedy **

• Set ε equal to how often we want to move randomly. 

• With probability 1 - ε, choose estimated best move. 

• With probability ε, choose a random move.

**function approximation: **when impossible to explore all states

approximating Q(s, a), often by a function combining various features, rather than storing one value for every state-action pair 


## unsupervised learning 

given input data without any additional feedback, learn patterns

**clustering **

organizing a set of objects into groups in such a way that similar objects tend to be in the same group

• Genetic research 

• Image segmentation 

• Market research 

• Medical imaging 

• Social network analysis.

k-means clustering 

algorithm for clustering data based on repeatedly assigning points to clusters and updating those clusters' centers


# Neural Networks

• Neurons are connected to and receive electrical signals from other neurons. 

• Neurons process input signals and can be activated.

**artificial neural network **

mathematical model for learning inspired by biological neural networks

• Model mathematical function from inputs to outputs based on the structure and parameters of the network. 

• Allows for learning the network's parameters based on data.

**activation functions**

Gives 0 before a certain threshold is reached and 1 after the threshold is reached.

Create some threshold based on value of hypothesis function: _h(x₁, x₂)_ = _w₀ + w₁x₁ + w₂x₂_,

**step function**

g(x) = 1 if x>=0 else 0

**logistic sigmoid**

g(x) = e^x / (e^x + 1)

**Rectified Linear Unit(ReLU)**

g(x) = max(0, x)

**gradient descent**

an algorithm for minimizing loss when training neural networks. (very slow)



*   Start with a random choice of weights. This is our naive starting place, where we don’t know how much we should weight each input.
*   Repeat:
    *   Calculate the gradient based on all data points that will lead to decreasing loss. Ultimately, the gradient is a vector (a sequence of numbers).
    *   Update weights according to the gradient.

**Stochastic Gradient Descent**

the gradient is calculated based on one point chosen at random

**Mini-Batch Gradient Descent**

computes the gradient based on on a few points selected at random

**Perceptron**

• Only capable of learning linearly separable decision boundary

**multilayer neural network **

artificial neural network with an input layer, an output layer, and at least one hidden layer

**backpropagation **

algorithm for training neural networks with hidden layers



*   Start with a random choice of weights. 
*   Repeat: 
    *   Calculate error for output layer. 
    *   For each layer, starting with output layer, and moving inwards towards earliest hidden layer: 
        *   Propagate error back one layer. 
        *   Update weights.

**deep neural networks **

neural network with multiple hidden layers


## overfitting

**dropout **

when training neural network, temporarily removing units — selected at random — from a neural network to prevent over-reliance on certain units


## TensorFlow

playground.tensorflow.org


## computer vision 

computational methods for analyzing and understanding digital images

**image convolution **

applying a filter that adds each pixel value of an image to its neighbors, weighted according to a kernel matrix

**pooling**

reducing the size of an input by sampling from regions in the input

**max-pooling** pooling by choosing the maximum value in each region

**convolutional neural network **

neural networks that use convolution, usually for analyzing images


## feed-forward neural network 

neural network that has connections only in one direction

input -> network -> output

**recurrent neural network neural **

network that generates output that feeds back into its own inputs

input -> network -> (back to) network -> output


# Natural Language Processing 

• automatic summarization 	

• information extraction 

• language identification 

• machine translation 

• named entity recognition 

• speech recognition 

• text classification 

• word sense disambiguation 

**formal grammar **

a system of rules for generating sentences in a language

**Context-Free Grammar**

NP → N | D N

VP → V | V NP

S → NP VP

**n-gram **

a contiguous sequence of n items from a sample of text

**word n-gram **

a contiguous sequence of n words from a sample of text

**tokenization **

the task of splitting a sequence of characters into pieces (tokens)

**bag-of-words **

model model that represents text as an unordered collection of words

**Naive Bayes**

Naive Bayes is a technique that’s can be used in sentiment analysis with the bag-of-words model

**additive smoothing **

adding a value α to each value in our distribution to smooth the data

**Laplace smoothing **

adding 1 to each value in our distribution: pretending we've seen each value one more time than we actually have

**information retrieval **

the task of finding relevant documents in response to a user query

**topic modeling **

models for discovering the topics for a set of documents

**function words **

words that have little meaning on their own, but are used to grammatically connect other words

**content words **

words that carry meaning independently

**inverse document frequency **

measure of how common or rare a word is across documents

log(TotalDocuments/NumDocumentsContaining(word))

**tf-idf **

ranking of what words are important in a document by multiplying term frequency (TF) by inverse document frequency (IDF)

**information extraction **

the task of extracting knowledge from documents

**distribution representation **

representation of meaning distributed across multiple values

**word2vec **

model for generating word vectors

**skip-gram architecture **

neural network architecture for predicting context words given a target word
